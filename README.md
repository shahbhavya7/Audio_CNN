# ğŸµ  Resona - Neural Audio Mapper

<div align="center">
  

  
  [![Built with PyTorch](https://img.shields.io/badge/Built%20with-PyTorch-EE4C2C?style=for-the-badge&logo=pytorch)](https://pytorch.org)
  [![Frontend Framework](https://img.shields.io/badge/Frontend-Next.js-000000?style=for-the-badge&logo=next.js)](https://nextjs.org)
  [![API Framework](https://img.shields.io/badge/API-FastAPI-009688?style=for-the-badge&logo=fastapi)](https://fastapi.tiangolo.com)
  [![Audio Processing](https://img.shields.io/badge/Audio-Librosa-FF6B6B?style=for-the-badge&logo=python)](https://librosa.org)
  [![Serverless Training](https://img.shields.io/badge/Training-Modal-4ECDC4?style=for-the-badge&logo=cloud)](https://modal.com)
  
  **An intelligent audio analysis system with real-time CNN deep learning visualizations**
  
  *Experience AI-powered audio classification with interactive deep learning insights and stunning visual representations of neural network processing!*
  
</div>

## ğŸŒŸ Features

<div align="center">
  
  | ğŸ§  **Deep CNN Architecture** | ğŸ¨ **Interactive Visualization** | âš¡ **Real-time Processing** |
  |:----------------------------:|:--------------------------------:|:---------------------------:|
  | ResNet-inspired residual blocks | Live feature map heatmaps | Serverless GPU inference |
  | 50-class audio classification | Spectrogram visualizations | <100ms response times |
  
  | ğŸ¯ **Advanced Audio Processing** | ğŸš€ **Modern Web Interface** | ğŸ“Š **Training Analytics** |
  |:-------------------------------:|:---------------------------:|:------------------------:|
  | Mel spectrogram conversion | Next.js responsive frontend | TensorBoard integration |
  | Mixup data augmentation | Drag-and-drop audio upload | Performance metrics tracking |
  
</div>

## âœ¨ What makes Resona special?

- **ğŸ§  Advanced CNN Architecture** - ResNet-inspired model with residual blocks for superior audio classification
- **ğŸ¨ Real-time Feature Visualization** - Interactive heatmaps showing how the AI "sees" and processes audio
- **ğŸµ Comprehensive Audio Support** - Classifies 50 different environmental sound categories from ESC-50 dataset
- **âš¡ Serverless Inference** - Lightning-fast GPU-powered predictions using Modal platform
- **ğŸ¨ Modern Web Interface** - Sleek Next.js frontend with intuitive drag-and-drop functionality
- **ğŸ“Š Deep Learning Insights** - Visualize mel spectrograms, feature maps, and prediction confidence
- **ğŸ”§ Production-Ready API** - FastAPI backend with automatic documentation and validation
- **ğŸ“ˆ Advanced Training** - Sophisticated data augmentation with mixup and spectrogram masking

## ğŸš€ Quick Start

### Prerequisites

```bash
# Node.js 18+ for frontend
node --version

# Python 3.8+ for backend
python --version

# Modal CLI for serverless deployment
pip install modal
```

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/resona-audio-visualizer.git
   cd resona-audio-visualizer
   ```

2. **Backend Setup**
   ```bash
   # Install Python dependencies
   pip install torch torchvision torchaudio
   pip install fastapi uvicorn
   pip install librosa soundfile
   pip install modal
   pip install numpy pandas tqdm
   ```

3. **Frontend Setup**
   ```bash
   # Install Node.js dependencies
   npm install next react react-dom
   npm install @types/node typescript
   npm install tailwindcss
   ```

4. **Deploy Model to Modal**
   ```bash
   # Train the model (optional - pre-trained available)
   modal run train.py
   
   # Deploy inference endpoint
   modal deploy main.py
   ```

5. **Start the Application**
   ```bash
   # Backend (if running locally)
   python main.py
   
   # Frontend
   npm run dev
   ```

## ğŸ› ï¸ Tech Stack

<div align="center">
  
  ![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
  ![Next.js](https://img.shields.io/badge/Next.js-000000?style=for-the-badge&logo=next.js&logoColor=white)
  ![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white)
  ![Modal](https://img.shields.io/badge/Modal-4ECDC4?style=for-the-badge&logo=cloud&logoColor=white)
  ![Librosa](https://img.shields.io/badge/Librosa-FF6B6B?style=for-the-badge&logo=python&logoColor=white)
  ![TensorBoard](https://img.shields.io/badge/TensorBoard-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)
  
</div>

### Core Technologies

- **Deep Learning**: PyTorch (Neural network implementation and training)
- **Frontend**: Next.js + TypeScript (Modern React framework with SSR)
- **Backend API**: FastAPI (High-performance Python web framework)
- **Serverless Platform**: Modal (GPU-powered serverless inference)
- **Audio Processing**: Librosa + torchaudio (Advanced audio analysis and transformation)
- **Visualization**: TensorBoard (Training metrics and model analysis)
- **Dataset**: ESC-50 (Environmental Sound Classification - 50 categories)

## ğŸ’¡ How It Works

### System Architecture

```mermaid
graph TD
    A[ğŸµ Audio Upload] --> B[ğŸ”§ Preprocessing]
    B --> C[ğŸ“Š Mel Spectrogram]
    C --> D[ğŸ§  ResNet CNN]
    D --> E[ğŸ¯ Classification]
    D --> F[ğŸ¨ Feature Maps]
    E --> G[ğŸ“ˆ Confidence Scores]
    F --> H[ğŸŒˆ Heatmap Visualization]
    G --> I[ğŸ–¥ï¸ Next.js Frontend]
    H --> I
    
    J[â˜ï¸ Modal Serverless] --> K[âš¡ GPU Inference]
    K --> L[ğŸ“¡ FastAPI Endpoint]
    L --> M[ğŸ”„ Real-time Response]
    
    style A fill:#4ECDC4,stroke:#333,stroke-width:2px,color:#fff
    style D fill:#FF6B6B,stroke:#333,stroke-width:2px,color:#fff
    style I fill:#45B7D1,stroke:#333,stroke-width:2px,color:#fff
    style J fill:#96CEB4,stroke:#333,stroke-width:2px,color:#fff
```

### Processing Pipeline

1. **ğŸµ Audio Input** - User uploads audio file through Next.js interface
2. **ğŸ”§ Preprocessing** - Audio normalization, resampling to 44.1kHz, mono conversion
3. **ğŸ“Š Spectrogram Generation** - Convert audio to mel spectrogram using librosa transforms
4. **ğŸ§  CNN Inference** - Process through ResNet-inspired architecture on Modal GPU
5. **ğŸ¯ Classification** - Predict among 50 environmental sound categories
6. **ğŸ¨ Feature Extraction** - Extract intermediate CNN feature maps for visualization
7. **ğŸ“Š Visualization** - Generate interactive heatmaps and confidence charts
8. **ğŸ–¥ï¸ Display** - Present results in beautiful Next.js interface

## ğŸ® Key Components Breakdown

### ğŸ§  ResNet-Inspired CNN (model.py)
```python
Architecture Highlights:
â”œâ”€â”€ Input Layer: 1-channel mel spectrogram (128 mel bands)
â”œâ”€â”€ Conv1: 7x7 convolution + BatchNorm + ReLU + MaxPool
â”œâ”€â”€ Layer1: 3x ResidualBlocks (64 channels)
â”œâ”€â”€ Layer2: 4x ResidualBlocks (128 channels, stride=2)
â”œâ”€â”€ Layer3: 6x ResidualBlocks (256 channels, stride=2)
â”œâ”€â”€ Layer4: 3x ResidualBlocks (512 channels, stride=2)
â”œâ”€â”€ GlobalAvgPool: Adaptive pooling to 1x1
â”œâ”€â”€ Dropout: 50% regularization
â””â”€â”€ FC Layer: 512 â†’ 50 classes
```

**Key Features:**
- **Residual Connections** - Skip connections prevent vanishing gradients
- **Batch Normalization** - Stable training and faster convergence
- **Adaptive Pooling** - Handles variable input sizes efficiently
- **Feature Map Extraction** - Returns intermediate activations for visualization

### ğŸ¯ Serverless Inference Engine (main.py)
- **Modal Platform Integration** - Automatic GPU provisioning and scaling
- **FastAPI Endpoints** - RESTful API with automatic documentation
- **Audio Processing Pipeline** - Base64 encoding, format conversion, preprocessing
- **Feature Map Generation** - Real-time extraction of CNN intermediate layers
- **Response Optimization** - Efficient serialization and data compression

### ğŸš€ Advanced Training System (train.py)
- **ESC-50 Dataset** - Comprehensive environmental sound classification
- **Data Augmentation** - Mixup, frequency masking, time masking
- **One Cycle Learning** - Advanced learning rate scheduling
- **TensorBoard Logging** - Comprehensive training metrics
- **Model Checkpointing** - Automatic best model saving

### ğŸ¨ Modern Frontend (Next.js)
- **Responsive Design** - Mobile-first, adaptive layouts
- **Real-time Visualization** - Interactive audio waveforms and spectrograms
- **Feature Map Display** - Dynamic heatmap generation and rendering
- **Drag-and-Drop Upload** - Intuitive file handling with progress indicators

## ğŸ“Š Model Architecture Details

### ResidualBlock Structure
```python
ResidualBlock Components:
â”œâ”€â”€ Conv2d(3x3) â†’ BatchNorm â†’ ReLU
â”œâ”€â”€ Conv2d(3x3) â†’ BatchNorm
â”œâ”€â”€ Shortcut Connection (1x1 conv if dimensions change)
â””â”€â”€ Addition â†’ ReLU activation
```

### Audio Feature Processing
```python
Audio Pipeline:
â”œâ”€â”€ Raw Audio (44.1kHz, mono)
â”œâ”€â”€ Mel Spectrogram (128 mel bands, 1024 FFT)
â”œâ”€â”€ Amplitude to dB conversion
â”œâ”€â”€ Data Augmentation (training only)
â”‚   â”œâ”€â”€ Frequency Masking (30 freq bands)
â”‚   â”œâ”€â”€ Time Masking (80 time steps)
â”‚   â””â”€â”€ Mixup (Î²=0.2, 30% probability)
â””â”€â”€ Normalization for CNN input
```





## ğŸ¯ Performance Metrics

<div align="center">
  
  | Metric | Performance |
  |:------:|:-----------:|
  | ğŸ¯ **Classification Accuracy** | 85%+ on ESC-50 test set |
  | âš¡ **Inference Speed** | <100ms per audio file |
  | ğŸ§  **Model Parameters** | ~23M trainable parameters |
  | ğŸ’¾ **Model Size** | ~90MB saved checkpoint |
  | ğŸš€ **Training Time** | ~2 hours on A100G GPU |
  | ğŸ“Š **Feature Maps** | 16 visualization layers |
  
</div>

## ğŸ¨ Visualization Features

### Real-time Visualizations
- **ğŸ“Š Mel Spectrogram** - Time-frequency representation of input audio
- **ğŸŒˆ Feature Map Heatmaps** - Layer-wise CNN activation visualization  
- **ğŸ“ˆ Confidence Charts** - Top-3 prediction probabilities
- **ğŸµ Waveform Display** - Original audio signal visualization
- **ğŸ¯ Classification Results** - Detailed prediction breakdown

### Interactive Elements
- **ğŸ” Zoom Controls** - Explore spectrogram details
- **â¯ï¸ Audio Playback** - Listen to uploaded audio
- **ğŸ“± Responsive Design** - Works on mobile and desktop
- **ğŸ¨ Dynamic Theming** - Dark/light mode support





## ğŸ¤ Contributing

Contributions are welcome to enhance the Resona Audio Visualizer!

1. **ğŸ´ Fork the repository**

2. **ğŸŒŸ Create your feature branch**
   ```bash
   git checkout -b feature/EnhancedVisualization
   ```

3. **ğŸ’» Commit your changes**
   ```bash
   git commit -m 'Add advanced feature map clustering'
   ```

4. **ğŸš€ Push to the branch**
   ```bash
   git push origin feature/EnhancedVisualization
   ```

5. **ğŸ“¬ Open a Pull Request**


## ğŸ§ª Experiments and Extensions

### Possible Enhancements

- **ğŸ¼ Music Classification** - Extend to musical instrument and genre recognition
- **ğŸ“± Real-time Processing** - WebRTC integration for live audio classification
- **ğŸŒ Multi-language Support** - Internationalization for global accessibility  
- **ğŸ¯ Custom Dataset Training** - Support for user-provided audio datasets
- **ğŸ”Š Audio Synthesis** - Generate audio samples from feature representations
- **ğŸ“Š Advanced Analytics** - Detailed model interpretability tools
- **ğŸ¨ 3D Visualizations** - WebGL-powered 3D feature map rendering
- **âš¡ Edge Deployment** - ONNX model conversion for client-side inference


## ğŸ”® Future Roadmap

- **ğŸŒ Web Audio API** - Direct browser microphone integration
- **ğŸ“± Mobile App** - React Native version for iOS and Android
- **ğŸ® Interactive Demos** - Educational audio processing tutorials
- **ğŸ”Š Spatial Audio** - 3D audio classification and visualization
- **ğŸ¤– Model Compression** - Quantization and pruning for faster inference
- **ğŸ“Š A/B Testing** - Experiment framework for model improvements
- **ğŸ” Privacy Features** - On-device processing options
- **ğŸ¨ Custom Themes** - User-customizable visualization styles



---


*Last updated: September 2025*